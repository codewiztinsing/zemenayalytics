Zemen Analytics
================

Zemen Analytics is a Django-based analytics service for tracking blog activity and surfacing API-driven insights about:

- Blog performance over time (views and creations)
- Top blogs, authors, and countries
- Blog view analytics grouped by users or countries
- Compressive time-series aggregates powered by Celery and django-celery-beat

The application is containerized with Docker and exposes a REST API documented via OpenAPI/Swagger using `drf-spectacular`.


## Technology Stack

- **Backend**: Django, Django REST Framework
- **Task queue**: Celery
- **Scheduler**: django-celery-beat
- **Broker / result backend**: Redis
- **Database**: With PostgreSQL-ready configuration via environment variables
- **API documentation**: drf-spectacular (OpenAPI 3)
- **Testing**: pytest, pytest-django, factory-boy
- **Containers**: Docker, Docker Compose


## Core Domain Models

- **Country**
  - Fields: `code`, `name`, `continent`, `created_at`, `updated_at`
  - Used for country-based analytics and filtering.

- **Author**
  - Wraps Django `User` and inherits common timestamp fields from an abstract `BaseModel`.
  - All `Blog` records are required to point to an `Author` (not directly to `User`).

- **Blog**
  - Represents a blog post.
  - Key fields: `title`, `author` (`Author` FK), `country` (`Country` FK), `created_at`, `updated_at`.
  - Ordering is by `-created_at` for analytics-friendly time semantics.

- **BlogView**
  - Represents a single view of a blog.
  - Key fields: `blog`, `user` (`User` FK, nullable), `viewed_at`, plus inherited `created_at`/`updated_at` via `BaseModel` (for analytics time filtering).

- **Time Series Aggregates**
  - `BlogViewTimeSeriesAggregate` and `BlogCreationTimeSeriesAggregate`
  - Single-table, compressive time-series design:
    - `granularity`: day / week / month / year
    - `time_bucket`: the start of the period
    - Foreign keys (optionally null): `blog`, `author`, `country`
    - Aggregates: `view_count`, `blog_count`, etc.
  - Used by performance analytics to answer queries without scanning raw rows.


## Key Features

- **Centralized, colored logging**
  - Configured in `config/settings/base.py` with `colorlog` (when installed).
  - Console output uses colors (INFO green, WARNING yellow, ERROR red) and file logging to `logs/django.log` for easier troubleshooting.

- **Comprehensive analytics**
  - Blog Views Analytics (`/analytics/blog-views/`):
    - Group by `country` or `user`.
    - Filters via a JSON filter tree (dynamic AND/OR/NOT, eq/lt/lte/gt/gte/contains/in).
    - Time range filtering against `created_at`.
  - Top Analytics (`/analytics/top/`):
    - Top blogs, users, or countries by total views.
    - Optional date range and filters; paginated responses.
  - Performance Analytics (`/analytics/performance/`):
    - Time-series performance for a user or all users.
    - Compare by `day`, `week`, `month`, or `year`.
    - `x` = period label + number of blogs created.
    - `y` = views during the period.
    - `z` = growth/decline percentage vs previous period.

- **Compressive time series with Celery**
  - Celery tasks aggregate raw `Blog` and `BlogView` data into time-series tables at multiple granularities.
  - `django-celery-beat` schedules periodic aggregation (e.g., hourly, daily).
  - A custom `backfill_time_series` management command can generate aggregates for historical data, with an option to clear existing aggregates first.

- **Clean OpenAPI documentation**
  - Schema generated by `drf-spectacular`.
  - Custom hooks remove `/api/` and `/api/v1` from paths.
  - Schema components are cleaned to keep pagination and response schemas while removing noisy request schemas.


## Running the Project

### Requirements

- Docker and Docker Compose
- Optional: Python 3.11+ and a virtualenv if you want to run Django directly without Docker


### Environment Variables

Create a `.env` file in the project root. The typical variables include:

- `DJANGO_SETTINGS_MODULE` (e.g. `config.settings.local` or `config.settings.production`)
- `SECRET_KEY`
- `DEBUG`
- Database settings (for PostgreSQL in containers), for example:
  - `POSTGRES_DB`
  - `POSTGRES_USER`
  - `POSTGRES_PASSWORD`
  - `POSTGRES_HOST`
  - `POSTGRES_PORT`
- Celery / Redis:
  - `CELERY_BROKER_URL`
  - `CELERY_RESULT_BACKEND`

Defaults are configured so local development can fall back to SQLite when no PostgreSQL env vars are provided.

### Using Docker Compose (recommended)

The `docker-compose.yml` file defines services like:

- `dev` – Django development server using `Dockerfile.dev`, with code mounted from the host.
- `celery_worker` – Celery worker using the production `Dockerfile`.
- `celery_beat` – Celery beat scheduler using the production `Dockerfile`.
- `backfill_time_series` – One-off service that runs `python manage.py backfill_time_series --clear` (for production or controlled backfills).
- Database and Redis services as needed.

Typical workflow:

1. Build and start containers:
   - `docker compose up --build`
2. Apply migrations:
   - `docker compose exec web python manage.py migrate`
2. Access the API and admin:
   - Django dev server: `http://127.0.0.01:8000/api/docs/`
   - Admin: `http://127.0.0.01:8000/admin/`
   - API schema / docs (depending on URLs configured): for example, `/schema/`, `/docs/`


## Time Series and Celery

- **Backfilling aggregates**
  - Use the management command:
    - `python manage.py backfill_time_series --clear`
      - `--clear` removes existing aggregates before recalculating.
  - Intended to be run either from:
    - The `backfill_time_series` Docker service, or
    - Manually inside the `dev` container.

- **Celery worker & beat**
  - Worker:
    - `celery -A config worker --loglevel=info`
  - Beat:
    - `celery -A config beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler`
  - In Docker, these are separate services; logs will show when periodic tasks are dispatched and executed.

- **Testing that beat aggregates correctly (conceptual)**
  - Seed a known small dataset (few blogs and views for specific dates).
  - Temporarily set beat intervals to short periods in `django_celery_beat` for local dev.
  - Watch `celery_beat` and `celery_worker` logs for task execution.
  - Inspect aggregate tables (via admin or DB) to verify counts and time buckets are as expected.


## Populating Sample Data

There is a custom management command for populating demo data:

- `python manage.py populate_data`
  - Uses factory-boy to generate countries, users/authors, blogs, and blog views.
  - Controlled via environment variables (e.g. `FACTORY_COUNTRIES`, `FACTORY_BLOGS`, etc.).
  - Command-line options override env defaults and respect `0` (meaning “do not populate this table”).
  - Safely uses `get_or_create` to avoid duplicate users and includes retry logic for unique country codes.

After populating data, you can:

- Hit the analytics endpoints to get non-empty responses.
- Run `backfill_time_series` to create aggregates for historical data.


## Testing

Tests are written with `pytest` and `pytest-django`. Typical commands (inside the web container):

- Run all tests:
  - `pytest`

- Run only analytics tests:
  - `pytest apps/analytics/tests`

- Reuse the test database for faster runs:
  - `pytest --reuse-db`

If migrations change (e.g. new fields on `Country`, `Blog`, or `BlogView`), you may need to force a fresh test database:

- `pytest --reuse-db --create-db`


## Logging and Debugging

- Logs:
  - Console logs are colorized when `colorlog` is available.
  - File logs are written to `logs/django.log` with a verbose formatter.
- Application logging:
  - A central logger (`config/logger.py`) exposes a logger named `"apps"`.
  - Views and services log key events, such as:
    - Incoming requests and query parameters.
    - Validation errors and filter parsing problems.
    - Aggregation counts and high-level performance metrics.

Use the logs to trace analytics requests and Celery activity during development and troubleshooting.


## Conventions and Notes

- All analytics time range filters are based on **`created_at`** for `Blog`/`BlogView` when computing analytics, providing a single, consistent timeline.
- `Blog.author` is always an `Author` instance, not a bare `User`.
- Raw models for time series have been removed in favor of single-table aggregate models backed by Celery-based background jobs.

For deeper architectural details, see the in-repo documentation in the `docs/` directory (for example, the compressive time-series design notes and time-series population guide).